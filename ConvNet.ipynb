{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, padding, stride, filter_size):\n",
    "        self.num_filters = num_filters\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.filter_size = filter_size\n",
    "    def _initialize_vars(self, input_size):\n",
    "        n_examples, h_prev, w_prev, c_prev = input_size\n",
    "        if self.padding == 'same':\n",
    "            n_h = h_prev\n",
    "            n_w = w_prev\n",
    "            self.pad = True\n",
    "            self.pad_val_h_l = ((n_h - 1)*self.stride + self.filter_size - n_h)//2\n",
    "            self.pad_val_h_r = ((n_h - 1)*self.stride + self.filter_size - n_h) - self.pad_val_h_l\n",
    "            self.pad_val_w_l = ((n_w - 1)*self.stride + self.filter_size - n_w)//2\n",
    "            self.pad_val_w_r = ((n_w - 1)*self.stride + self.filter_size - n_w) - self.pad_val_w_l\n",
    "            #print(\"Same Padding Vals!\", self.pad_val_h_l, self.pad_val_h_r)\n",
    "        else:\n",
    "            n_h = (h_prev - self.filter_size) // self.stride + 1\n",
    "            n_w = (w_prev - self.filter_size) // self.stride + 1\n",
    "            self.pad = False\n",
    "        n_c = self.num_filters\n",
    "        self.kernels = np.random.randn(self.num_filters, self.filter_size*self.filter_size*c_prev)\n",
    "        self.op_size = (n_examples, n_h, n_w, n_c)\n",
    "        self.bias = np.zeros(self.num_filters)\n",
    "        self.update_vars = [self.kernels, self.bias]\n",
    "    def forward(self, input_vol):\n",
    "        #print(input_vol.shape)\n",
    "        n_ex = input_vol.shape[0]\n",
    "        self.n_ex = n_ex\n",
    "        self.n_c_prev = input_vol.shape[3]\n",
    "        _, n_h, n_w, n_c = self.op_size\n",
    "        self.op_vol = np.zeros((n_ex, n_h, n_w, n_c))\n",
    "        n_h_prev = input_vol.shape[1]\n",
    "        n_w_prev = input_vol.shape[2]\n",
    "        self.n_h_prev = n_h_prev\n",
    "        self.n_w_prev = n_w_prev\n",
    "        if self.pad == True:\n",
    "            input_vol = np.pad(input_vol, ((0,0),(self.pad_val_h_l, self.pad_val_h_r),(self.pad_val_w_l, self.pad_val_w_r),(0,0)),'constant', constant_values = 0)\n",
    "            n_h_prev += (self.pad_val_h_l + self.pad_val_h_r)\n",
    "            n_w_prev += (self.pad_val_w_l + self.pad_val_w_r)\n",
    "            #print(\"Same Padding!\", n_h_prev, n_w_prev, \"Shape:\", input_vol.shape)\n",
    "        self.n_h_prev_pad = n_h_prev\n",
    "        self.n_w_prev_pad = n_w_prev\n",
    "        num_patches_w = (n_w_prev - self.filter_size) // self.stride + 1\n",
    "        num_patches_h = (n_h_prev - self.filter_size) // self.stride + 1\n",
    "        #print(num_patches_w,num_patches_h)\n",
    "        self.im2col_cache = np.zeros((n_ex, num_patches_h*num_patches_w, self.filter_size*self.filter_size*input_vol.shape[3]))\n",
    "        for example_num in range(n_ex):\n",
    "            self.im2col_cache[example_num] = im2col(input_vol[example_num, :, :, :], self.filter_size, self.stride)\n",
    "            #print(self.im2col_cache[example_num].shape)\n",
    "            for h in range(n_h):\n",
    "                for w in range(n_w):\n",
    "                    curr_slice = self.im2col_cache[example_num,(n_h*h)+w]\n",
    "                    for c in range(n_c):\n",
    "                        self.op_vol[example_num, h, w, c] = np.dot(curr_slice, self.kernels[c]) + self.bias[c]\n",
    "        return self.op_vol\n",
    "    def backward(self, da_next):\n",
    "        da_prev = np.zeros((self.n_ex, self.n_h_prev_pad, self.n_w_prev_pad, self.n_c_prev))\n",
    "        d_kernel = np.zeros(self.kernels.shape)\n",
    "        d_bias = np.zeros(self.bias.shape)\n",
    "        for example_num in range(self.n_ex):\n",
    "            for h in range(da_next.shape[1]):\n",
    "                for w in range(da_next.shape[2]):\n",
    "                    for c in range(da_next.shape[3]):\n",
    "                        d_kernel[c] += da_next[example_num, h, w, c]*self.im2col_cache[example_num, h*da_next.shape[1]+w,:]\n",
    "                        da_prev[example_num, h*self.stride:h*self.stride+self.filter_size, w*self.stride:w*self.stride+self.filter_size, :] += (da_next[example_num, h, w, c]*(self.kernels[c].reshape((self.filter_size, self.filter_size, self.n_c_prev))))\n",
    "        d_bias += np.sum(np.sum(np.sum(da_next, axis=2, keepdims = True),axis=1, keepdims = True),axis=0, keepdims = True).reshape(d_bias.shape)\n",
    "        #print(da_prev)\n",
    "        #raise ValueError(\"Wait!\")\n",
    "        if self.pad == True:\n",
    "            return [da_prev[:, self.pad_val_h_l:-self.pad_val_h_r, self.pad_val_w_l: -self.pad_val_w_r, :], d_kernel, d_bias]\n",
    "        else:\n",
    "            return [da_prev, d_kernel, d_bias]\n",
    "    def update(self, update_list):\n",
    "        if len(update_list) > 2:\n",
    "            raise ValueError(\"Check number of parameters\")\n",
    "        self.kernels -= update_list[0]\n",
    "        self.bias -= update_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(array, filter_size, stride):\n",
    "    \n",
    "    h, w, c = array.shape\n",
    "    #print(array.shape)\n",
    "    num_patches_w = (w - filter_size) // stride + 1\n",
    "    num_patches_h = (h - filter_size) // stride + 1\n",
    "    col = np.zeros((num_patches_h*num_patches_w, filter_size*filter_size*c))\n",
    "    row_index = 0\n",
    "    for i in range(num_patches_h):\n",
    "        col_index = 0\n",
    "        for j in range(num_patches_w):\n",
    "            col[num_patches_h*i+j] = np.copy(array[row_index:row_index+filter_size, col_index:col_index+filter_size,:].reshape(-1))\n",
    "            col_index += stride\n",
    "        row_index += stride\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = ConvLayer(10, 'same', 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1._initialize_vars((None, 5, 5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_vol = np.random.randn(4,5,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 5, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1.forward(ip_vol).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = np.random.randn(4,5,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, name = 'sigmoid', leaky_relu_rate = None):\n",
    "        self.act = name\n",
    "        self.leaky_relu_rate = leaky_relu_rate\n",
    "    def _initialize_vars(self, input_size):\n",
    "        self.op_size = input_size\n",
    "        self.update_vars = []\n",
    "    def forward(self, prev_layer):\n",
    "        if self.act == 'sigmoid':\n",
    "            self.op = 1.0 / (1.0 + np.exp(-prev_layer))\n",
    "        elif self.act == 'relu':\n",
    "            self.op = np.maximum(0, prev_layer)\n",
    "            self.op_mask = prev_layer>0\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.op = np.maximum(prev_layer*self.leaky_relu_rate, prev_layer)\n",
    "            self.op_mask = prev_layer>0 + ((prev_layer<0)*self.leaky_relu_rate)\n",
    "        elif activation == 'tanh':\n",
    "            self.op = np.tanh(prev_layer)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation String\")\n",
    "        return self.op\n",
    "    def backward(self, da_next):\n",
    "        if self.act == 'sigmoid':\n",
    "            self.da_prev = self.op * (1-self.op) * da_next\n",
    "        elif self.act == 'relu':\n",
    "            self.da_prev = self.op_mask * da_next\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.da_prev = self.op_mask * da_next\n",
    "        elif activation == 'tanh':\n",
    "            self.da_prev = 1 - np.square(self.op)\n",
    "        return [self.da_prev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def __init__(self, padding, stride, pool_size):\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.pool_size = pool_size\n",
    "    def _initialize_vars(self, input_size):\n",
    "        n_ex, n_h_prev, n_w_prev, n_c = input_size\n",
    "        if self.padding == 'same':\n",
    "            n_h = n_h_prev\n",
    "            n_w = n_w_prev\n",
    "            self.pad = True\n",
    "            self.pad_val_h = ((n_h - 1)*self.stride + self.pool_size - n_h)//2\n",
    "            self.pad_val_w = ((n_w - 1)*self.stride + self.pool_size - n_w)//2\n",
    "        else:\n",
    "            n_h = (n_h_prev - self.pool_size)//self.stride + 1\n",
    "            n_w = (n_w_prev - self.pool_size)//self.stride + 1\n",
    "            self.pad = False\n",
    "        self.op_size = (n_ex, n_h, n_w, n_c)\n",
    "        self.update_vars = []\n",
    "    def forward(self, input_vol):\n",
    "        n_ex, n_h_prev, n_w_prev, n_c = input_vol.shape\n",
    "        self.n_ex = n_ex\n",
    "        self.n_c = n_c\n",
    "        _, n_h, n_w, _ = self.op_size\n",
    "        if self.pad == True:\n",
    "            input_vol = np.pad(input_vol, ((0,0),(self.pad_val_h, self.pad_val_h),(self.pad_val_w, self.pad_val_w),(0,0)), 'constant', constant_values=0)\n",
    "        self.op_vol = np.zeros((n_ex, n_h, n_w, n_c))\n",
    "        self.ip_mask = np.zeros(input_vol.shape)\n",
    "        for e in range(n_ex):\n",
    "            for c in range(n_c):\n",
    "                for h in range(n_h):\n",
    "                    for w in range(n_w):\n",
    "                        self.op_vol[e,h,w,c] = np.max(input_vol[e,h*self.stride:h*self.stride+self.pool_size,w*self.stride:w*self.stride+self.pool_size,c])\n",
    "                        mask = input_vol[e,h*self.stride:h*self.stride+self.pool_size,w*self.stride:w*self.stride+self.pool_size,c] == self.op_vol[e,h,w,c]\n",
    "                        self.ip_mask[e,h*self.stride:h*self.stride+self.pool_size,w*self.stride:w*self.stride+self.pool_size,c] += mask                \n",
    "        return self.op_vol\n",
    "    def backward(self, da_next):\n",
    "        da_prev = self.ip_mask\n",
    "        for e in range(da_next.shape[0]):\n",
    "            for h in range(da_next.shape[1]):\n",
    "                for w in range(da_next.shape[2]):\n",
    "                    for c in range(da_next.shape[3]):\n",
    "                        da_prev[e,h*self.stride:h*self.stride+self.pool_size,w*self.stride:w*self.stride+self.pool_size,c] *= da_next[e,h,w,c]\n",
    "        if self.pad == True:\n",
    "            return [da_prev[:,self.pad_val_h:-self.pad_val_h, self.pad_val_w:-self.pad_val_w,:]]\n",
    "        else:\n",
    "            return [da_prev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = MaxPool('valid', 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1._initialize_vars((None,5,5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 2, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1.forward(ip_vol).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = Activation('relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1._initialize_vars((None, 5,5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 5, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1.forward(ip_vol).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 5.27213575e-02, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [7.95585519e-01, 7.18981675e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[8.62311303e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.09911683e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.04639377e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [3.73687427e-01, 5.12400596e-01, 1.28319781e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.26615232e+00],\n",
       "         [1.03785935e-04, 2.88910079e-02, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[0.00000000e+00, 2.98262023e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 3.39088168e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[3.74204983e-01, 0.00000000e+00, 3.45002668e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [2.13175151e+00, 5.44962639e-02, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 4.55426663e-01, 0.00000000e+00],\n",
       "         [1.03360753e-01, 0.00000000e+00, 4.15942253e-01],\n",
       "         [1.24134732e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 5.69603320e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.06904907e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[2.33977597e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.45353248e-02, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 2.90874418e-01],\n",
       "         [0.00000000e+00, 7.79890116e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.13284905e+00],\n",
       "         [6.74863881e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.03761910e+00, 9.05786512e-01],\n",
       "         [2.48222125e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[7.88475959e-01, 9.96312698e-01, 2.77386906e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[0.00000000e+00, 2.53281437e-01, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 2.36382595e-03],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 2.31462696e-02],\n",
       "         [5.30737617e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [1.17949084e+00, 1.15977708e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[1.45597570e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 2.94439918e-01, 1.84537925e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 8.57268576e-06, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [2.06292090e+00, 0.00000000e+00, 3.60578868e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "        [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1.backward(ip_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 5, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1.backward(ip_vol).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def _initialize_vars(self, input_size):\n",
    "        n_ex, n_h, n_w, n_c = input_size\n",
    "        self.op_size = (n_ex, n_h*n_w*n_c)\n",
    "        self.update_vars = []\n",
    "    def forward(self, input_vol):\n",
    "        self.prev_shape = input_vol.shape\n",
    "        n_ex, n_h, n_w, n_c = input_vol.shape\n",
    "        op_vol = np.reshape(input_vol,(n_ex, n_h*n_w*n_c))\n",
    "        return op_vol\n",
    "    def backward(self, da_next):\n",
    "        da_prev = da_next.reshape(self.prev_shape)\n",
    "        #print(\"Flatten:\",da_prev.shape)\n",
    "        return [da_prev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1._initialize_vars(ip_vol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 5, 3)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1.backward(F1.forward(ip_vol)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool:\n",
    "    def __init__(self, padding, stride, pool_size):\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.pool_size = pool_size\n",
    "    def _initialize_vars(self, input_size):\n",
    "        n_ex, n_h_prev, n_w_prev, n_c = input_size\n",
    "        if self.padding == 'same':\n",
    "            n_h = n_h_prev\n",
    "            n_w = n_w_prev\n",
    "            self.pad = True\n",
    "            self.pad_val_h = ((n_h - 1)*self.stride + self.pool_size - n_h)//2\n",
    "            self.pad_val_w = ((n_w - 1)*self.stride + self.pool_size - n_w)//2\n",
    "        else:\n",
    "            n_h = (n_h_prev - self.pool_size)//self.stride + 1\n",
    "            n_w = (n_w_prev - self.pool_size)//self.stride + 1\n",
    "            self.pad = False\n",
    "        self.op_size = (n_ex, n_h, n_w, n_c)\n",
    "        self.update_vars = []\n",
    "    def forward(self, input_vol):\n",
    "        n_ex, n_h_prev, n_w_prev, n_c = input_vol.shape\n",
    "        self.n_ex = n_ex\n",
    "        self.n_c = n_c\n",
    "        _, n_h, n_w, _ = self.op_size\n",
    "        if self.pad == True:\n",
    "            input_vol = np.pad(input_vol, ((0,0),(self.pad_val_h, self.pad_val_h),(self.pad_val_w, self.pad_val_w),(0,0)), 'constant', constant_values=0)\n",
    "        self.op_vol = np.zeros((n_ex, n_h, n_w, n_c))\n",
    "        self.ip_mask = np.ones(input_vol.shape)/(pool_size*pool_size)\n",
    "        for e in range(n_ex):\n",
    "            for c in range(n_c):\n",
    "                for h in range(n_h):\n",
    "                    for w in range(n_w):\n",
    "                        self.op_vol[e,h,w,c] = np.average(input_vol[e,h*self.stride:h*self.stride+self.pool_size,w*self.stride:w*self.stride+self.pool_size,c])\n",
    "                        mask = input_vol[e,h*self.stride:h*self.stride+self.pool_size,w*self.stride:w*self.stride+self.pool_size,c] == self.op_vol[e,h,w,c]\n",
    "                        self.ip_mask[e,h*self.stride:h*self.stride+self.pool_size,w*self.stride:w*self.stride+self.pool_size,c] += mask                \n",
    "        return self.op_vol\n",
    "    def backward(self, da_next):\n",
    "        da_prev = self.ip_mask\n",
    "        for e in range(da_next.shape[0]):\n",
    "            for h in range(da_next.shape[1]):\n",
    "                for w in range(da_next.shape[2]):\n",
    "                    for c in range(da_next.shape[3]):\n",
    "                        da_prev[e,h*self.stride:h*self.stride+self.pool_size,w*self.stride:w*self.stride+self.pool_size,c] *= da_next[e,h,w,c]\n",
    "        if self.pad == True:\n",
    "            return [da_prev[:,self.pad_val_h:-self.pad_val_h, self.pad_val_w:-self.pad_val_w,:]]\n",
    "        else:\n",
    "            return [da_prev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    def __init__(self, num_units):\n",
    "        self.num_units = num_units\n",
    "    def _initialize_vars(self, input_shape):\n",
    "        self.ip_shape = input_shape\n",
    "        n_ex, n_prev = input_shape\n",
    "        self.weights = np.random.randn(n_prev, self.num_units)\n",
    "        self.biases = np.zeros((1, self.num_units))\n",
    "        self.op_size = (n_ex, self.num_units)\n",
    "        self.update_vars = [self.weights, self.biases]\n",
    "    def forward(self, input_vol):\n",
    "        self.input_vol = input_vol\n",
    "        op = np.dot(self.input_vol, self.weights) + self.biases\n",
    "        return op\n",
    "    def backward(self, da_next):\n",
    "        d_weights = np.dot(self.input_vol.T, da_next)\n",
    "        d_biases = np.sum(da_next, axis = 0, keepdims=True)\n",
    "        da_prev = np.dot(da_next, self.weights.T)\n",
    "        return [da_prev, d_weights, d_biases]\n",
    "    def update(self, update_list):\n",
    "        if len(update_list) > 2:\n",
    "            raise ValueError(\"Check number of parameters\")\n",
    "        self.weights -= update_list[0]\n",
    "        self.biases -= update_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC1 = FullyConnected(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC1._initialize_vars((None, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 45)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC1.forward(np.random.rand(15,20)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 45)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC1.backward(np.random.rand(15, 45))[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input_shape):\n",
    "        self.layers = []\n",
    "        self.updates = []\n",
    "        self.update_vals = []\n",
    "        self.update_layers = []\n",
    "        self.last_shape = input_shape\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        layer._initialize_vars(self.last_shape)\n",
    "        self.last_shape = layer.op_size\n",
    "        self.update_layers.append(len(layer.update_vars))\n",
    "    def summary(self):\n",
    "        for layer in self.layers:\n",
    "            layer.summary()\n",
    "    def train(self, x_data, y_data, batch_size = 16, num_epochs = 100, loss = \"mse\", optimizer = \"SGD\", learning_rate = 0.000001, shuffle = False, momentum = 0.9, print_period = 1):\n",
    "        num_batches = y_data.shape[0] // batch_size\n",
    "        for epoch in range(num_epochs):\n",
    "            if shuffle:\n",
    "                x_data, y_data = shuffle(x_data, y_data)\n",
    "            for batch_num in range(num_batches + 1):\n",
    "                if batch_num != num_batches:\n",
    "                    X = x_data[batch_num*batch_size:batch_num*batch_size+batch_size]\n",
    "                    Y = y_data[batch_num*batch_size:batch_num*batch_size+batch_size]\n",
    "                else:\n",
    "                    if batch_num*batch_size == y_data.shape[0]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        X = x_data[batch_num*batch_size:]\n",
    "                        Y = y_data[batch_num*batch_size:]\n",
    "                data_feed = X\n",
    "                for layer in self.layers:\n",
    "                    data_feed = layer.forward(data_feed)\n",
    "                if loss == 'mse':\n",
    "                    cost = np.mean(np.square((data_feed - Y)), axis = 0)/2\n",
    "                    d_cost = (data_feed - Y)/ Y.shape[0]\n",
    "                elif loss == 'crossentropy':\n",
    "                    cost = -np.sum(Y*np.log(data_feed) + (1-Y)*np.log(1-data_feed))\n",
    "                    d_cost = -Y/data_feed + (1-Y)/(1-data_feed)\n",
    "                current_deriv = d_cost\n",
    "                for layer in reversed(self.layers):\n",
    "                    #print(current_deriv.shape)\n",
    "                    items = layer.backward(current_deriv)\n",
    "                    for index, item in enumerate(items):\n",
    "                        if index == 0:\n",
    "                            current_deriv = item\n",
    "                            #print(layer)\n",
    "                        else:\n",
    "                            self.update_vals.append(item)\n",
    "                if optimizer == 'SGD':\n",
    "                    self.updates = [learning_rate*u for u in self.update_vals]\n",
    "                elif optimizer == 'SGD with Momentum':\n",
    "                    pass\n",
    "                param_count = 0\n",
    "                num_layers = len(self.layers) - 1\n",
    "                for index, layer in enumerate(reversed(self.layers)):\n",
    "                    num_params_for_layer = self.update_layers[num_layers - index]\n",
    "                    if num_params_for_layer > 0:\n",
    "                        layer.update(self.updates[param_count:param_count+num_params_for_layer])\n",
    "                    param_count += num_params_for_layer\n",
    "            if epoch % print_period == 0:\n",
    "                data_feed = x_data\n",
    "                for layer in self.layers:\n",
    "                    data_feed = layer.forward(data_feed)\n",
    "                error = np.mean(np.square(data_feed - y_data))\n",
    "                accuracy = np.sum(np.equal(data_feed,y_data))/y_data.shape[0]\n",
    "                print(\"Epoch {}: Error: {} Accuracy: {}\".format(epoch, error, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(20, 10, 10, 3)\n",
    "y = np.random.rand(20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = Model((None, 5,5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.add(ConvLayer(5,'same',1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.add(ConvLayer(1,'same',1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 5, 5, 1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.last_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Error: 179.68235558616266 Accuracy: 0.0\n",
      "Epoch 1: Error: 177.62121920328306 Accuracy: 0.0\n",
      "Epoch 2: Error: 175.58294768737576 Accuracy: 0.0\n",
      "Epoch 3: Error: 173.56744764481346 Accuracy: 0.0\n",
      "Epoch 4: Error: 171.57462583206643 Accuracy: 0.0\n",
      "Epoch 5: Error: 169.60438915570268 Accuracy: 0.0\n",
      "Epoch 6: Error: 167.65664467238764 Accuracy: 0.0\n",
      "Epoch 7: Error: 165.73129958888453 Accuracy: 0.0\n",
      "Epoch 8: Error: 163.82826126205416 Accuracy: 0.0\n",
      "Epoch 9: Error: 161.9474371988549 Accuracy: 0.0\n",
      "Epoch 10: Error: 160.08873505634278 Accuracy: 0.0\n",
      "Epoch 11: Error: 158.25206264167153 Accuracy: 0.0\n",
      "Epoch 12: Error: 156.43732791209231 Accuracy: 0.0\n",
      "Epoch 13: Error: 154.64443897495417 Accuracy: 0.0\n",
      "Epoch 14: Error: 152.8733040877035 Accuracy: 0.0\n",
      "Epoch 15: Error: 151.1238316578845 Accuracy: 0.0\n",
      "Epoch 16: Error: 149.39593024313893 Accuracy: 0.0\n",
      "Epoch 17: Error: 147.68950855120622 Accuracy: 0.0\n",
      "Epoch 18: Error: 146.00447543992337 Accuracy: 0.0\n",
      "Epoch 19: Error: 144.34073991722497 Accuracy: 0.0\n",
      "Epoch 20: Error: 142.69821114114333 Accuracy: 0.0\n",
      "Epoch 21: Error: 141.07679841980834 Accuracy: 0.0\n",
      "Epoch 22: Error: 139.47641121144744 Accuracy: 0.0\n",
      "Epoch 23: Error: 137.89695912438586 Accuracy: 0.0\n",
      "Epoch 24: Error: 136.33835191704623 Accuracy: 0.0\n",
      "Epoch 25: Error: 134.80049949794903 Accuracy: 0.0\n",
      "Epoch 26: Error: 133.2833119257122 Accuracy: 0.0\n",
      "Epoch 27: Error: 131.7866994090514 Accuracy: 0.0\n",
      "Epoch 28: Error: 130.31057230677987 Accuracy: 0.0\n",
      "Epoch 29: Error: 128.8548411278084 Accuracy: 0.0\n",
      "Epoch 30: Error: 127.41941653114557 Accuracy: 0.0\n",
      "Epoch 31: Error: 126.00420932589748 Accuracy: 0.0\n",
      "Epoch 32: Error: 124.6091304712678 Accuracy: 0.0\n",
      "Epoch 33: Error: 123.23409107655793 Accuracy: 0.0\n",
      "Epoch 34: Error: 121.87900240116686 Accuracy: 0.0\n",
      "Epoch 35: Error: 120.54377585459116 Accuracy: 0.0\n",
      "Epoch 36: Error: 119.22832299642508 Accuracy: 0.0\n",
      "Epoch 37: Error: 117.93255553636043 Accuracy: 0.0\n",
      "Epoch 38: Error: 116.65638533418674 Accuracy: 0.0\n",
      "Epoch 39: Error: 115.39972439979104 Accuracy: 0.0\n",
      "Epoch 40: Error: 114.16248489315808 Accuracy: 0.0\n",
      "Epoch 41: Error: 112.9445791243702 Accuracy: 0.0\n",
      "Epoch 42: Error: 111.74591955360732 Accuracy: 0.0\n",
      "Epoch 43: Error: 110.5664187911471 Accuracy: 0.0\n",
      "Epoch 44: Error: 109.40598959736468 Accuracy: 0.0\n",
      "Epoch 45: Error: 108.2645448827329 Accuracy: 0.0\n",
      "Epoch 46: Error: 107.14199770782224 Accuracy: 0.0\n",
      "Epoch 47: Error: 106.03826128330076 Accuracy: 0.0\n",
      "Epoch 48: Error: 104.95324896993414 Accuracy: 0.0\n",
      "Epoch 49: Error: 103.88687427858572 Accuracy: 0.0\n",
      "Epoch 50: Error: 102.83905087021643 Accuracy: 0.0\n",
      "Epoch 51: Error: 101.80969255588484 Accuracy: 0.0\n",
      "Epoch 52: Error: 100.79871329674711 Accuracy: 0.0\n",
      "Epoch 53: Error: 99.80602720405712 Accuracy: 0.0\n",
      "Epoch 54: Error: 98.83154853916622 Accuracy: 0.0\n",
      "Epoch 55: Error: 97.87519171352352 Accuracy: 0.0\n",
      "Epoch 56: Error: 96.9368712886757 Accuracy: 0.0\n",
      "Epoch 57: Error: 96.01650197626704 Accuracy: 0.0\n",
      "Epoch 58: Error: 95.11399863803946 Accuracy: 0.0\n",
      "Epoch 59: Error: 94.22927628583251 Accuracy: 0.0\n",
      "Epoch 60: Error: 93.3622500815834 Accuracy: 0.0\n",
      "Epoch 61: Error: 92.51283533732686 Accuracy: 0.0\n",
      "Epoch 62: Error: 91.68094751519533 Accuracy: 0.0\n",
      "Epoch 63: Error: 90.86650222741888 Accuracy: 0.0\n",
      "Epoch 64: Error: 90.06941523632511 Accuracy: 0.0\n",
      "Epoch 65: Error: 89.28960245433936 Accuracy: 0.0\n",
      "Epoch 66: Error: 88.52697994398449 Accuracy: 0.0\n",
      "Epoch 67: Error: 87.78146391788106 Accuracy: 0.0\n",
      "Epoch 68: Error: 87.05297073874719 Accuracy: 0.0\n",
      "Epoch 69: Error: 86.34141691939868 Accuracy: 0.0\n",
      "Epoch 70: Error: 85.64671912274892 Accuracy: 0.0\n",
      "Epoch 71: Error: 84.96879416180894 Accuracy: 0.0\n",
      "Epoch 72: Error: 84.30755899968736 Accuracy: 0.0\n",
      "Epoch 73: Error: 83.66293074959047 Accuracy: 0.0\n",
      "Epoch 74: Error: 83.03482667482214 Accuracy: 0.0\n",
      "Epoch 75: Error: 82.42316418878386 Accuracy: 0.0\n",
      "Epoch 76: Error: 81.82786085497479 Accuracy: 0.0\n",
      "Epoch 77: Error: 81.24883438699169 Accuracy: 0.0\n",
      "Epoch 78: Error: 80.68600264852893 Accuracy: 0.0\n",
      "Epoch 79: Error: 80.13928365337848 Accuracy: 0.0\n",
      "Epoch 80: Error: 79.60859556543005 Accuracy: 0.0\n",
      "Epoch 81: Error: 79.09385669867079 Accuracy: 0.0\n",
      "Epoch 82: Error: 78.59498551718562 Accuracy: 0.0\n",
      "Epoch 83: Error: 78.11190063515704 Accuracy: 0.0\n",
      "Epoch 84: Error: 77.64452081686512 Accuracy: 0.0\n",
      "Epoch 85: Error: 77.19276497668764 Accuracy: 0.0\n",
      "Epoch 86: Error: 76.75655217909993 Accuracy: 0.0\n",
      "Epoch 87: Error: 76.335801638675 Accuracy: 0.0\n",
      "Epoch 88: Error: 75.93043272008342 Accuracy: 0.0\n",
      "Epoch 89: Error: 75.54036493809343 Accuracy: 0.0\n",
      "Epoch 90: Error: 75.16551795757093 Accuracy: 0.0\n",
      "Epoch 91: Error: 74.80581159347933 Accuracy: 0.0\n",
      "Epoch 92: Error: 74.46116581087975 Accuracy: 0.0\n",
      "Epoch 93: Error: 74.1315007249309 Accuracy: 0.0\n",
      "Epoch 94: Error: 73.81673660088916 Accuracy: 0.0\n",
      "Epoch 95: Error: 73.51679385410843 Accuracy: 0.0\n",
      "Epoch 96: Error: 73.23159305004035 Accuracy: 0.0\n",
      "Epoch 97: Error: 72.9610549042341 Accuracy: 0.0\n",
      "Epoch 98: Error: 72.70510028233655 Accuracy: 0.0\n",
      "Epoch 99: Error: 72.46365020009212 Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "m2.train(np.random.rand(10,5,5,3),np.random.rand(10,5,5,1),batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
